{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../house-prices-advanced-regression-techniques/train.csv\")\n",
    "test_data = pd.read_csv(\"../house-prices-advanced-regression-techniques/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n",
      "0   1          60       RL         65.0       WD        Normal     208500\n",
      "1   2          20       RL         80.0       WD        Normal     181500\n",
      "2   3          60       RL         68.0       WD        Normal     223500\n",
      "3   4          70       RL         60.0       WD       Abnorml     140000\n",
      "(1460, 81)\n",
      "(1459, 80)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 79)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n",
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "all_features[numerical_features] = all_features[numerical_features].apply(lambda x: (x - x.mean())/(x.std()))\n",
    "all_features[numerical_features] = all_features[numerical_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 330)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = pd.get_dummies(all_features, dummy_na=True)\n",
    "all_features = all_features.astype(np.float32)\n",
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "def data_iter(data_array, batch_size, is_train):\n",
    "    dataset = data.TensorDataset(*data_array)\n",
    "    dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num = train_data.shape[0]\n",
    "\n",
    "train_features = torch.tensor(all_features.iloc[:train_data_num, :].values, dtype=torch.float32)\n",
    "test_features = torch.tensor(all_features.iloc[train_data_num:, :].values, dtype=torch.float32)\n",
    "train_label = torch.tensor(train_data.iloc[:, -1].values, dtype=torch.float32)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataloader = data_iter((train_features, train_label.reshape(-1, 1)), batch_size=batch_size, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_feature = train_features.shape[1]\n",
    "\n",
    "def xavier_init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_feature, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "model = MyModel(input_feature)\n",
    "model.apply(xavier_init_weights)\n",
    "\n",
    "optimer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18/500 [00:00<00:05, 83.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614759843.4191781\n",
      "513584432.3945205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 37/500 [00:00<00:05, 87.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281868010.9589041\n",
      "102295188.33972603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 55/500 [00:00<00:05, 85.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39273773.89589041\n",
      "26930389.30410959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 73/500 [00:00<00:04, 87.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23696249.468493152\n",
      "21998808.89863014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 91/500 [00:01<00:05, 81.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20762737.293150686\n",
      "19722160.679452054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 119/500 [00:01<00:04, 86.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18849690.915068492\n",
      "18081037.852054793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 128/500 [00:01<00:04, 86.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17501200.635616437\n",
      "16908312.89863014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 155/500 [00:01<00:04, 83.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17001476.339726027\n",
      "15965970.147945205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 175/500 [00:02<00:03, 87.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15514937.161643835\n",
      "15188515.06849315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 195/500 [00:02<00:03, 90.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14828018.991780821\n",
      "14527207.473972602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 215/500 [00:02<00:03, 90.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14241677.78630137\n",
      "14063351.06849315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 235/500 [00:02<00:02, 89.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13727840.460273972\n",
      "13488380.767123288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 253/500 [00:02<00:02, 88.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13260563.112328768\n",
      "13135040.263013698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 271/500 [00:03<00:02, 89.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12961176.964383561\n",
      "12739380.252054794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 291/500 [00:03<00:02, 90.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12558612.208219178\n",
      "12532338.487671234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 311/500 [00:03<00:02, 89.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12565034.115068493\n",
      "12181537.852054795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 330/500 [00:03<00:02, 79.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12055698.367123287\n",
      "11970292.591780823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 359/500 [00:04<00:01, 87.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855114.750684932\n",
      "11723761.78630137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 377/500 [00:04<00:01, 87.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11621985.939726027\n",
      "11559375.682191782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 395/500 [00:04<00:01, 87.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11442571.419178082\n",
      "11379527.156164384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 414/500 [00:04<00:00, 88.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11327502.619178083\n",
      "11717975.14520548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 433/500 [00:04<00:00, 89.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11153312.942465754\n",
      "11140841.150684932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 453/500 [00:05<00:00, 90.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11034983.287671233\n",
      "10962826.334246576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 473/500 [00:05<00:00, 88.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11171221.293150686\n",
      "10857353.49041096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 491/500 [00:05<00:00, 88.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10859466.695890412\n",
      "10759745.183561644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 87.10it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 500\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "log = {\"train_loss\":[]}\n",
    "\n",
    "for epoch in tqdm(range(epoch_num)):\n",
    "    total_loss = 0\n",
    "    for X, y in dataloader:\n",
    "        y_hat = model(X)\n",
    "        l = loss(y_hat, y.reshape(-1, 1))\n",
    "        optimer.zero_grad()\n",
    "        l.backward()\n",
    "        optimer.step()\n",
    "        total_loss += l.item()\n",
    "    \n",
    "    total_loss /= train_features.shape[0]\n",
    "    log[\"train_loss\"].append(total_loss)\n",
    "    if epoch%10 == 0:\n",
    "        print(total_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "in_features = train_features.shape[1]\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential(nn.Linear(in_features,1))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_rmse(net, features, labels):\n",
    "    # 为了在取对数时进一步稳定该值，将小于1的值设置为1\n",
    "    clipped_preds = torch.clamp(net(features), 1, float('inf'))\n",
    "    rmse = torch.sqrt(loss(torch.log(clipped_preds),\n",
    "                           torch.log(labels)))\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_features, train_labels, test_features, test_labels,\n",
    "          num_epochs, learning_rate, weight_decay, batch_size):\n",
    "    train_ls, test_ls = [], []\n",
    "    train_iter = data_iter((train_features, train_labels), batch_size, True)\n",
    "    # 这里使用的是Adam优化算法\n",
    "    optimizer = torch.optim.Adam(net.parameters(),\n",
    "                                 lr = learning_rate,\n",
    "                                 weight_decay = weight_decay)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for X, y in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += l.item()\n",
    "        train_ls.append(log_rmse(net, train_features, train_labels))\n",
    "        print(total_loss)\n",
    "        if test_labels is not None:\n",
    "            test_ls.append(log_rmse(net, test_features, test_labels))\n",
    "    return train_ls, test_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,\n",
    "           batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net()\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        print(f'折{i + 1}，训练log rmse{float(train_ls[-1]):f}, '\n",
    "              f'验证log rmse{float(valid_ls[-1]):f}')\n",
    "    return train_l_sum / k, valid_l_sum / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1168])) that is different to the input size (torch.Size([1168, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736747591680.0\n",
      "702423980032.0\n",
      "683098701824.0\n",
      "644280659968.0\n",
      "633627729920.0\n",
      "605345622016.0\n",
      "593776306176.0\n",
      "553800847360.0\n",
      "529852409856.0\n",
      "506667507712.0\n",
      "497439578112.0\n",
      "479125628928.0\n",
      "459017316352.0\n",
      "442792247296.0\n",
      "422855540736.0\n",
      "413948108800.0\n",
      "394569330688.0\n",
      "372553231360.0\n",
      "363968534528.0\n",
      "358403741696.0\n",
      "338520762368.0\n",
      "325576989696.0\n",
      "325537537024.0\n",
      "304816826880.0\n",
      "293318575104.0\n",
      "289311612928.0\n",
      "270889105920.0\n",
      "262550835200.0\n",
      "253933498368.0\n",
      "243052049920.0\n",
      "235914853376.0\n",
      "226863068672.0\n",
      "225653523456.0\n",
      "213580354048.0\n",
      "206204158208.0\n",
      "203655651840.0\n",
      "209634328064.0\n",
      "191427623936.0\n",
      "184975895296.0\n",
      "185184509952.0\n",
      "175490392832.0\n",
      "177194488832.0\n",
      "167488952576.0\n",
      "169233084416.0\n",
      "160406641792.0\n",
      "157948566016.0\n",
      "154282614784.0\n",
      "153389716480.0\n",
      "155087692288.0\n",
      "149814015488.0\n",
      "146357076992.0\n",
      "148239803136.0\n",
      "150376041984.0\n",
      "144046568960.0\n",
      "168666627072.0\n",
      "141628624384.0\n",
      "137597031936.0\n",
      "146048007424.0\n",
      "152802543104.0\n",
      "134941509888.0\n",
      "130461767168.0\n",
      "128631552128.0\n",
      "129331160832.0\n",
      "130009022464.0\n",
      "132768347648.0\n",
      "128991219200.0\n",
      "125917409792.0\n",
      "128615966720.0\n",
      "130880489216.0\n",
      "123797021056.0\n",
      "123936404736.0\n",
      "125835724800.0\n",
      "125585389824.0\n",
      "138583781376.0\n",
      "125607833600.0\n",
      "124207844096.0\n",
      "122434635776.0\n",
      "125371835392.0\n",
      "121761339136.0\n",
      "120974263296.0\n",
      "126191412480.0\n",
      "125550660352.0\n",
      "120636081280.0\n",
      "123740891392.0\n",
      "122853519360.0\n",
      "121618984704.0\n",
      "126296112640.0\n",
      "123081030912.0\n",
      "121919160320.0\n",
      "122272166912.0\n",
      "124596326144.0\n",
      "122372570880.0\n",
      "121325350400.0\n",
      "123495196928.0\n",
      "124065732352.0\n",
      "124277699328.0\n",
      "121585386496.0\n",
      "136811743488.0\n",
      "122925309184.0\n",
      "123335934208.0\n",
      "折1，训练log rmse0.409382, 验证log rmse0.391080\n",
      "717325492224.0\n",
      "710852960256.0\n",
      "662477950976.0\n",
      "648405751808.0\n",
      "611945801728.0\n",
      "593153935360.0\n",
      "576547031040.0\n",
      "553443942400.0\n",
      "536716439552.0\n",
      "504406898688.0\n",
      "479116065792.0\n",
      "477386237952.0\n",
      "456872719360.0\n",
      "437628309504.0\n",
      "415236165632.0\n",
      "395632781312.0\n",
      "393008388096.0\n",
      "368955471872.0\n",
      "351517089792.0\n",
      "343571132416.0\n",
      "333416431616.0\n",
      "323630493696.0\n",
      "307029575680.0\n",
      "302828139520.0\n",
      "285811467264.0\n",
      "273961601024.0\n",
      "262873888768.0\n",
      "252963600896.0\n",
      "250190038016.0\n",
      "241454804480.0\n",
      "230412210176.0\n",
      "223902636544.0\n",
      "218994668544.0\n",
      "208498545664.0\n",
      "208213231616.0\n",
      "199358079488.0\n",
      "189787583488.0\n",
      "185169651712.0\n",
      "178668508928.0\n",
      "181460680704.0\n",
      "173045018624.0\n",
      "170159048704.0\n",
      "161350997504.0\n",
      "161473210368.0\n",
      "160060079616.0\n",
      "154597758464.0\n",
      "156715748864.0\n",
      "152988667392.0\n",
      "144388304896.0\n",
      "142480619776.0\n",
      "144709744384.0\n",
      "141151744512.0\n",
      "138049355520.0\n",
      "135390576384.0\n",
      "136878557952.0\n",
      "131577008384.0\n",
      "135861088256.0\n",
      "129528488192.0\n",
      "130954918400.0\n",
      "137715273984.0\n",
      "131055179008.0\n",
      "124221313024.0\n",
      "126462262016.0\n",
      "123908023552.0\n",
      "122345826304.0\n",
      "120790300032.0\n",
      "121312744704.0\n",
      "120238084480.0\n",
      "119644949504.0\n",
      "127145056256.0\n",
      "126729936128.0\n",
      "118664154496.0\n",
      "122209985280.0\n",
      "122797620224.0\n",
      "122946328832.0\n",
      "118259352576.0\n",
      "120880993536.0\n",
      "117364304128.0\n",
      "120466972672.0\n",
      "117619891968.0\n",
      "125322753024.0\n",
      "120630473216.0\n",
      "118318538752.0\n",
      "119046254592.0\n",
      "116000070784.0\n",
      "118594852096.0\n",
      "121627458048.0\n",
      "116745741824.0\n",
      "117535987712.0\n",
      "117036186624.0\n",
      "127172669440.0\n",
      "115583296256.0\n",
      "117131730688.0\n",
      "119284785920.0\n",
      "118306985472.0\n",
      "125275217664.0\n",
      "117630579456.0\n",
      "118811918592.0\n",
      "116009436672.0\n",
      "118492567040.0\n",
      "折2，训练log rmse0.399078, 验证log rmse0.430149\n",
      "718061565952.0\n",
      "687295350784.0\n",
      "656340260864.0\n",
      "631643834368.0\n",
      "607715528704.0\n",
      "597425448960.0\n",
      "566259109888.0\n",
      "548899405824.0\n",
      "528563786752.0\n",
      "506165413888.0\n",
      "484328907776.0\n",
      "460577875968.0\n",
      "441070774272.0\n",
      "425522681856.0\n",
      "404266715136.0\n",
      "395130128384.0\n",
      "383090551808.0\n",
      "361288665088.0\n",
      "349347864576.0\n",
      "344908201984.0\n",
      "320566080512.0\n",
      "308580211712.0\n",
      "302364367872.0\n",
      "286873020416.0\n",
      "275718290944.0\n",
      "268006265856.0\n",
      "257291108352.0\n",
      "248181311488.0\n",
      "238996609024.0\n",
      "233385396736.0\n",
      "225472646656.0\n",
      "220960193024.0\n",
      "211005600768.0\n",
      "203252689920.0\n",
      "201438911488.0\n",
      "192590005760.0\n",
      "186785377024.0\n",
      "183566998016.0\n",
      "179412966400.0\n",
      "172441695744.0\n",
      "166241323008.0\n",
      "161905287680.0\n",
      "171905358336.0\n",
      "158521654016.0\n",
      "149780792064.0\n",
      "149759480576.0\n",
      "146033496832.0\n",
      "144222115072.0\n",
      "141427318784.0\n",
      "140657177600.0\n",
      "135975330048.0\n",
      "133584961280.0\n",
      "135838997504.0\n",
      "137612410880.0\n",
      "129798087680.0\n",
      "128166707456.0\n",
      "124267534208.0\n",
      "127303869696.0\n",
      "127457087744.0\n",
      "123575109632.0\n",
      "120829201152.0\n",
      "123410515712.0\n",
      "120148638720.0\n",
      "119087494912.0\n",
      "117113521152.0\n",
      "118505894144.0\n",
      "120294161152.0\n",
      "120987321856.0\n",
      "119613200896.0\n",
      "114564594176.0\n",
      "118904626688.0\n",
      "115382629888.0\n",
      "115627231232.0\n",
      "119294043136.0\n",
      "113457817088.0\n",
      "114350066176.0\n",
      "112164222336.0\n",
      "115204349184.0\n",
      "119892871936.0\n",
      "113156435200.0\n",
      "116267901952.0\n",
      "112133135872.0\n",
      "117199549184.0\n",
      "113374361600.0\n",
      "129721321216.0\n",
      "112312976128.0\n",
      "114907599360.0\n",
      "119919658496.0\n",
      "113919097856.0\n",
      "112426581248.0\n",
      "112351247616.0\n",
      "112734770688.0\n",
      "116373350144.0\n",
      "121510161152.0\n",
      "113500150272.0\n",
      "111649935104.0\n",
      "117624308992.0\n",
      "114111511296.0\n",
      "113314717952.0\n",
      "112365555200.0\n",
      "折3，训练log rmse0.403259, 验证log rmse0.413147\n",
      "744336627712.0\n",
      "717623928832.0\n",
      "700845588480.0\n",
      "661676922880.0\n",
      "643768866816.0\n",
      "617208627200.0\n",
      "585321969664.0\n",
      "573841434624.0\n",
      "549926287360.0\n",
      "524714625024.0\n",
      "519929032704.0\n",
      "506224525312.0\n",
      "474315743232.0\n",
      "454269011968.0\n",
      "433347935232.0\n",
      "419499776000.0\n",
      "398831884288.0\n",
      "395928892416.0\n",
      "382859415552.0\n",
      "359101805568.0\n",
      "353014982656.0\n",
      "344396223488.0\n",
      "324496041984.0\n",
      "310980952064.0\n",
      "308340777984.0\n",
      "290203516928.0\n",
      "284683384832.0\n",
      "273016524800.0\n",
      "262841851904.0\n",
      "252519800832.0\n",
      "240805793792.0\n",
      "242534777344.0\n",
      "228486393344.0\n",
      "220349040640.0\n",
      "220463688704.0\n",
      "217467893248.0\n",
      "212403345408.0\n",
      "196813088768.0\n",
      "190622965248.0\n",
      "191196706816.0\n",
      "180085405440.0\n",
      "176811065600.0\n",
      "191750638592.0\n",
      "175396791808.0\n",
      "175487746560.0\n",
      "163689738496.0\n",
      "168105897216.0\n",
      "162174574080.0\n",
      "154145977344.0\n",
      "153837070848.0\n",
      "151717924864.0\n",
      "147674162176.0\n",
      "145302674944.0\n",
      "149990779136.0\n",
      "146397066240.0\n",
      "140340086784.0\n",
      "138909930496.0\n",
      "135977531264.0\n",
      "139412020992.0\n",
      "133797802496.0\n",
      "132782337792.0\n",
      "150834216704.0\n",
      "134057678848.0\n",
      "132039670016.0\n",
      "130412001536.0\n",
      "131505877760.0\n",
      "129078109696.0\n",
      "134611576320.0\n",
      "127099967488.0\n",
      "126798920704.0\n",
      "127446879744.0\n",
      "142422649600.0\n",
      "127144201984.0\n",
      "130726485248.0\n",
      "124203902848.0\n",
      "125572286208.0\n",
      "125053409792.0\n",
      "125029008384.0\n",
      "125227278848.0\n",
      "127889335552.0\n",
      "133704321280.0\n",
      "124318545920.0\n",
      "124222870016.0\n",
      "141047013632.0\n",
      "124962092032.0\n",
      "125015436544.0\n",
      "139028229888.0\n",
      "122681018112.0\n",
      "125262469888.0\n",
      "123251880192.0\n",
      "122882117632.0\n",
      "122996902912.0\n",
      "123361425664.0\n",
      "126545589760.0\n",
      "122905649408.0\n",
      "122410022912.0\n",
      "128448735744.0\n",
      "129318926592.0\n",
      "126056887040.0\n",
      "127399129600.0\n",
      "折4，训练log rmse0.410301, 验证log rmse0.397523\n",
      "744127889408.0\n",
      "694221035520.0\n",
      "667564861440.0\n",
      "646987032576.0\n",
      "611318108160.0\n",
      "590474395648.0\n",
      "572814895104.0\n",
      "549385056256.0\n",
      "526834116608.0\n",
      "507293892608.0\n",
      "483643037696.0\n",
      "466943997952.0\n",
      "449845839872.0\n",
      "440508870656.0\n",
      "417219263488.0\n",
      "396700534784.0\n",
      "384540577792.0\n",
      "371912768512.0\n",
      "356720775168.0\n",
      "337700617728.0\n",
      "327307416576.0\n",
      "320703950848.0\n",
      "308215389184.0\n",
      "294201820160.0\n",
      "287327998976.0\n",
      "277164274688.0\n",
      "262759709184.0\n",
      "260740222976.0\n",
      "247941028352.0\n",
      "241722434560.0\n",
      "229325191680.0\n",
      "223189818368.0\n",
      "215161058816.0\n",
      "208640457216.0\n",
      "208019745792.0\n",
      "198012280320.0\n",
      "197934097920.0\n",
      "187588582912.0\n",
      "179847936512.0\n",
      "179953289216.0\n",
      "171811911680.0\n",
      "165078159360.0\n",
      "165638394112.0\n",
      "161526558464.0\n",
      "163496535808.0\n",
      "155833140736.0\n",
      "147931979648.0\n",
      "149930252800.0\n",
      "146517777920.0\n",
      "143540639232.0\n",
      "141929010176.0\n",
      "138961692160.0\n",
      "137876505600.0\n",
      "134048320768.0\n",
      "133651327232.0\n",
      "133042919936.0\n",
      "132360002560.0\n",
      "130021154816.0\n",
      "133194921216.0\n",
      "129017120768.0\n",
      "139446985472.0\n",
      "124370053632.0\n",
      "123859162624.0\n",
      "125089480704.0\n",
      "128617229568.0\n",
      "131277170688.0\n",
      "120728942336.0\n",
      "120670219520.0\n",
      "122377336832.0\n",
      "120406110464.0\n",
      "119700382208.0\n",
      "118590387456.0\n",
      "117132309248.0\n",
      "118292534784.0\n",
      "118423223040.0\n",
      "117189206784.0\n",
      "117605682944.0\n",
      "117585247232.0\n",
      "116797742848.0\n",
      "118386612480.0\n",
      "118302833920.0\n",
      "116381631488.0\n",
      "116420510720.0\n",
      "116313113856.0\n",
      "116707475968.0\n",
      "118127025152.0\n",
      "118108159488.0\n",
      "115807561472.0\n",
      "118241811712.0\n",
      "117259255040.0\n",
      "115126036480.0\n",
      "115940484608.0\n",
      "116400551680.0\n",
      "115764738304.0\n",
      "119100318976.0\n",
      "115426862592.0\n",
      "117066343936.0\n",
      "118463696384.0\n",
      "118841873664.0\n",
      "116241078784.0\n",
      "折5，训练log rmse0.407971, 验证log rmse0.396066\n",
      "5-折验证: 平均训练log rmse: 0.405998, 平均验证log rmse: 0.405593\n"
     ]
    }
   ],
   "source": [
    "k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\n",
    "train_l, valid_l = k_fold(k, train_features, train_label, num_epochs, lr,\n",
    "                          weight_decay, batch_size)\n",
    "print(f'{k}-折验证: 平均训练log rmse: {float(train_l):f}, '\n",
    "      f'平均验证log rmse: {float(valid_l):f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
