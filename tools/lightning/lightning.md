# lightning

åœ¨è®¾ç½®ç§å­çš„æ—¶å€™ï¼Œéœ€è¦è®¾ç½®å¾ˆå¤šseedï¼ŒåŒ…æ‹¬å¦‚ä¸‹å†…å®¹

```python
import random
import numpy as np
import torch

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)            # CPU ä¸Šè®¾ç½®éšæœºç§å­
torch.cuda.manual_seed(SEED)       # GPU ä¸Šè®¾ç½®éšæœºç§å­
torch.cuda.manual_seed_all(SEED)   # å¤š GPU æ—¶æ‰€æœ‰ GPU çš„éšæœºç§å­

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

```

è€Œä½¿ç”¨pytorch_lightingåªéœ€è¦ä¸€è¡Œ

```python
from pytorch_lightning import seed_everything

seed = 42
seed_everything(seed, workers=True)

# ä¸‹é¢è¿™äº›ä¸seedæ— å…³ï¼Œå¦‚æœå¸Œæœ›èƒ½å¤Ÿå¤ç°ç»“æœï¼Œå°±éœ€è¦è¿™ä¸ª
# Ensure that all operations are deterministic on GPU (if used) for reproducibility
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```

`pl.LightningModule` 

pl.LightningModuleç»§æ‰¿äºnn.Module, åŒ…å«å¦‚ä¸‹äº”ä¸ªéƒ¨åˆ†ï¼š

1. Initialization (`__init__`), where we create all necessary parameters/models
2. Optimizers (`configure_optimizers`) where we create the optimizers, learning rate scheduler, etc.
3. Training loop (`training_step`) where we only have to define the loss calculation for a single batch (the loop of optimizer.zero_grad(), loss.backward() and optimizer.step(), as well as any logging/saving operation, is done in the background)
4. Validation loop (`validation_step`) where similarly to the training, we only have to define what should happen per step
5. Test loop (`test_step`) which is the same as validation, only on a test set.
6. `forward` function

ä¸€ä¸ªä¾‹å­å¦‚ä¸‹

```python

def create_model(model_name, model_hparams):
    if model_name in model_dict:
        return model_dict[model_name](**model_hparams)
    else:
        assert False, f'Unknown model name "{model_name}". Available models are: {str(model_dict.keys())}'

class CIFARModule(pl.LightningModule):
    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):
        """CIFARModule.

        Args:
            model_name: Name of the model/CNN to run. Used for creating the model (see function below)
            model_hparams: Hyperparameters for the model, as dictionary.
            optimizer_name: Name of the optimizer to use. Currently supported: Adam, SGD
            optimizer_hparams: Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.

        """
        # initæ“ä½œå†…å®¹ï¼š
        # 1.ä¿å­˜è¶…å‚æ•°ï¼Œsave_hyperparametersï¼Œè¿™æ ·è¿˜ä¼šç»™ä½ åˆ›å»ºä¸€ä¸ªself.hparams
        # 2.åˆ›å»ºæ¨¡å‹ï¼ˆå¯ä»¥æå‰å…ˆç”¨nn.Moduleåˆ›å»ºä¸€ä¸ªclassï¼‰å’Œloss
        # æ³¨æ„ï¼Œoptimizersä¸åœ¨è¿™é‡Œå®šä¹‰ï¼Œè€Œæ˜¯æœ‰ä¸€ä¸ªä¸“é—¨çš„å‡½æ•°
        # 3.å…¶ä½™æ“ä½œ
        super().__init__()
        # Exports the hyperparameters to a YAML file, and create "self.hparams" namespace
        self.save_hyperparameters()
        # Create model
        self.model = create_model(model_name, model_hparams)
        # Create loss module
        self.loss_module = nn.CrossEntropyLoss()
        # Example input for visualizing the graph in Tensorboard
        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)

    def forward(self, imgs):
        return self.model(imgs)

    def configure_optimizers(self):
        # åœ¨optimizersçš„å®šä¹‰ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘å®šä¹‰å¦‚ä¸‹å†…å®¹
        # 1.optimizer
        # 2.lr_schedulerï¼ˆå¯ä»¥æ²¡æœ‰ï¼‰ï¼Œåœ¨å®šä¹‰çš„æ—¶å€™ï¼Œå°½é‡ç”¨ReduceLROnPlateau
        # æ³¨æ„ï¼Œåœ¨è¿”å›çš„æ—¶å€™ï¼Œå°½é‡ä»¥åˆ—è¡¨çš„å½¢å¼è¿”å›è¿™ä¸¤ä¸ªä¸œè¥¿ï¼Œé’ˆå¯¹æœ‰å¤šä¸ªoptimizerçš„æ—¶å€™å¾ˆæœ‰ç”¨
        if self.hparams.optimizer_name == "Adam":
            # AdamW is Adam with a correct implementation of weight decay (see here
            # for details: https://arxiv.org/abs/1711.05101)
            optimizer = optim.AdamW(self.parameters(), **self.hparams.optimizer_hparams)
        elif self.hparams.optimizer_name == "SGD":
            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)
        else:
            assert False, f'Unknown optimizer: "{self.hparams.optimizer_name}"'

        # We will reduce the learning rate by 0.1 after 100 and 150 epochs
        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)
        # æ›´å¸¸ç”¨çš„æ–¹å¼å¦‚ä¸‹
        # è¿™ä¸ªä¸éœ€è¦æŒ‡å®šä»€ä¹ˆæ—¶å€™é™ä½å­¦ä¹ ç‡ï¼Œå®ƒä¼šè‡ªå·±æ ¹æ®çœ‹ä»€ä¹ˆæ—¶å€™æ”¹learning rate
        scheduler = {
		        "scheduler": torch.optim.lr_scheduler.ReduceLROnPlateau(
		            optimizer,
		            mode="min",              # ç›‘æ§çš„æŒ‡æ ‡è¶Šå°è¶Šå¥½ï¼ˆé€šå¸¸æ˜¯ val_lossï¼‰
		            factor=0.1,              # å­¦ä¹ ç‡ç¼©å°ä¸ºåŸæ¥çš„ 0.1 å€
		            patience=10,            # è¿ç»­10ä¸ªepochæ— æå‡å°±è°ƒæ•´
		            verbose=True            # æ‰“å°å­¦ä¹ ç‡
		        ),
		        "monitor": "val_loss",      # å…³é”®ï¼šä¸€å®šè¦self.log("val_loss")
		        "interval": "epoch",        # æ¯ä¸ª epoch æ£€æŸ¥ä¸€æ¬¡
		        "frequency": 1
		    }
        return [optimizer], [scheduler]

    def training_step(self, batch, batch_idx):
        # "batch" is the output of the training data loader.
        # æ³¨æ„training_stepéœ€è¦returnï¼Œå…¶ä½™stepä¸éœ€è¦
        imgs, labels = batch
        preds = self.model(imgs)
        loss = self.loss_module(preds, labels)
        acc = (preds.argmax(dim=-1) == labels).float().mean()

        # Logs the accuracy per epoch to tensorboard (weighted average over batches)
        self.log("train_acc", acc, on_step=False, on_epoch=True)
        self.log("train_loss", loss)
        return loss  # Return tensor to call ".backward" on

    def validation_step(self, batch, batch_idx):
        imgs, labels = batch
        preds = self.model(imgs)
        loss = self.loss_module(preds, labels)
        preds = self.model(imgs).argmax(dim=-1)
        acc = (preds.argmax(dim=-1) == labels).float().mean()
        # By default logs it per epoch (weighted average over batches)
        self.log("val_acc", acc)
        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        

    def test_step(self, batch, batch_idx):
        imgs, labels = batch
        preds = self.model(imgs).argmax(dim=-1)
        acc = (labels == preds).float().mean()
        # By default logs it per epoch (weighted average over batches), and returns it afterwards
        self.log("test_acc", acc)
```

```python
# Callbacks
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint

model_dict = {}

def create_model(model_name, model_hparams):
    if model_name in model_dict:
        return model_dict[model_name](**model_hparams)
    else:
        assert False, f'Unknown model name "{model_name}". Available models are: {str(model_dict.keys())}'
        
act_fn_by_name = {"tanh": nn.Tanh, "relu": nn.ReLU, "leakyrelu": nn.LeakyReLU, "gelu": nn.GELU}

def train_model(model_name, save_name=None, **kwargs):
    """Train model.

    Args:
        model_name: Name of the model you want to run. Is used to look up the class in "model_dict"
        save_name (optional): If specified, this name will be used for creating the checkpoint and logging directory.

    """
    if save_name is None:
        save_name = model_name

    # Create a PyTorch Lightning trainer with the generation callback
    trainer = pl.Trainer(
        default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),
        accelerator="auto",
        devices=1,
        max_epochs=180,
        callbacks=[
            ModelCheckpoint(
                save_weights_only=True, mode="max", monitor="val_acc"
            ),
            LearningRateMonitor("epoch"),
        ],
    )
    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard
    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need

    # Check whether pretrained model exists. If yes, load it and skip training
    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + ".ckpt")
    if os.path.isfile(pretrained_filename):
        print(f"Found pretrained model at {pretrained_filename}, loading...")
        # Automatically loads the model with the saved hyperparameters
        model = CIFARModule.load_from_checkpoint(pretrained_filename)
    else:
        pl.seed_everything(42)  # To be reproducible
        model = CIFARModule(model_name=model_name, **kwargs)
        trainer.fit(model, train_loader, val_loader)
        model = CIFARModule.load_from_checkpoint(
            trainer.checkpoint_callback.best_model_path
        )  # Load best checkpoint after training

    # Test best model on validation and test set
    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)
    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)
    result = {"test": test_result[0]["test_acc"], "val": val_result[0]["test_acc"]}

    return model, result
```

```python
resnet_model,resnet_results=train_model(
		model_name="ResNet",
		model_hparams={"num_classes": 10, 
									 "c_hidden":[16, 32, 64], 
									 "num_blocks":[3, 3, 3], 
									 "act_fn_name": "relu"
									 },
		optimizer_name="SGD",
		optimizer_hparams={"lr": 0.1, "momentum": 0.9, "weight_decay": 1e-4},
		)
```

ä½¿ç”¨tensorboard

```python
from pytorch_lightning.loggers import TensorBoardLogger
logger = TensorBoardLogger("logs", name="my_model")  # logs/my_model/
```

åœ¨trainerä¸­ï¼š

```python
trainer = pl.Trainer(
			logger=logger,
			max_epochs=50,
			# å¯é€‰ï¼šç”¨äºæ˜¾ç¤ºæ¨¡å‹å›¾ç»“æ„
			enable_model_summary=True
			)
```

åœ¨æ¨¡å‹ä¸­ï¼›

```python
def training_step(self, batch, batch_idx):
    x, y = batch
    logits = self(x)
    loss = F.cross_entropy(logits, y)

    # è®°å½•è®­ç»ƒ loss
    self.log("train/loss", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
    return loss
```

å…¶ä½™å¯é€‰é¡¹ï¼š

trainerå®šä¹‰ä»¥åï¼Œ`trainer.logger._log_graph = True` å¯ä»¥ç»˜åˆ¶æ¨¡å‹ç»“æ„å›¾

å¯ä»¥åˆ©ç”¨è¿™ä¸ªå‡½æ•°ï¼Œåœ¨æ¨¡å‹è®­ç»ƒåï¼Œæ‰“å°å‡ºæ¥

```python
def validation_epoch_end(self, outputs):
    accs = [x["valid/acc"] for x in outputs if "valid/acc" in x]
    avg_acc = torch.stack(accs).mean()
    print(f"Epoch {self.current_epoch} - Valid Acc: {avg_acc:.4f}")

```

å®Œæ•´æµç¨‹å¦‚ä¸‹ï¼š

- è‡ªå·±å®šä¹‰å¥½æ¨¡å‹å’Œtrain/valid/test dataloader
- ç¼–å†™pl.LightningModule
    - é¦–å…ˆç¼–å†™initçš„æ—¶å€™ï¼Œæœ‰å‡ ä¸ªä¸œè¥¿æ˜¯è§„èŒƒçš„ï¼š
        
        `model_name` `model_hparams`  `optimizer_name` `optimizer_hparams` 
        
        ä¹Ÿå°±æ˜¯è¯´ï¼Œä½ åªéœ€è¦æŒ‘é€‰å¥½ä½ çš„æ¨¡å‹å‚æ•°ã€optimizerçš„å‚æ•°å°±è¡Œäº†
        
        åœ¨å†™initçš„æ—¶å€™ï¼Œé¦–å…ˆç¬¬ä¸€æ­¥ï¼Œéœ€è¦è®²è¿™äº›å‚æ•°ä»¥`self.save_hyperparameters()` çš„æ–¹å¼è¿›è¡Œä¿å­˜ï¼Œæ–¹ä¾¿åç»­ç›´æ¥åˆ©ç”¨self.hparams.xxxè¿›è¡Œåˆ©ç”¨
        
        ç„¶åéœ€è¦è®¾å®šå¥½`model`å’Œ`loss`
        
    - å†™å®Œinitï¼Œç®€å•å†™å®Œforward
    - configure_optimizers
        
        é¦–å…ˆä½ è¦ç¡®å®šè‡ªå·±çš„optimizeræ˜¯ä»€ä¹ˆï¼Œå®šä¹‰optimizer(lrå’Œweight decay)
        
        <aside>
        ğŸ’¡
        
        ç”±äºåœ¨å¸¸è§„çš„æ–¹å¼ä¸­ï¼Œmodelã€lossã€optimizeréƒ½æ˜¯ä¸€èµ·å†™çš„ï¼Œè€Œåœ¨è¿™é‡Œoptimizerå´å¦å¤–æœ‰ä¸€ä¸ªå‡½æ•°æ¥å†™äº†ï¼Œæ‰€ä»¥è¦è®°ä½è¿™ç§å·®å¼‚
        
        </aside>
        
        ç„¶åå†™schedulerï¼Œæ³¨æ„è¿™é‡Œå»ºè®®ä½¿ç”¨`ReduceLROnPlateau`ï¼Œæƒ³å¥½è¦åœ¨schedulerä¸­éœ€è¦å“ªäº›ä¸œè¥¿ï¼šå»ºè®®çš„èŒƒå¼å¦‚ä¸‹
        
        ```python
        scheduler = {
            "scheduler": torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer=optimizer,
                mode='min',
                factor=0.1,
                patience=10,
                verbose=True,
            ),
            "monitor": "valid/loss",
            "interval": "epoch",
            "frequency": 1
        }
        return [optimizer], [scheduler]
        ```
        
    - å†™å‡ ä¸ªstep, åªéœ€è¦æ³¨æ„å¼€å¤´è¦å†™ä¸Šbatch å’Œ batch idx
    - å¦‚æœæ¯ä¸ªepochæƒ³è¦æœ‰è¾“å‡ºï¼Œè¿˜å¯ä»¥ç”¨on_validation_epoch_endï¼Œç»“åˆ`self.trainer.callback_metrics.get("valid/acc")`æ¥æ‰“å°ä¸€ä¸‹epochçš„ç»“æœ

- å†™trainå‡½æ•°
    - å†™trainer
        
        å†™trainerçš„æ—¶å€™ï¼Œå°±æ˜¯ä»å¤§å±€è§‚çš„è§’åº¦æ¥ç¼–å†™æ¨¡å‹ï¼Œé‚£ä¹ˆå°±åº”è¯¥è¦æƒ³å¦‚ä¸‹çš„ä¸œè¥¿
        
        **åœ°å€ï¼ˆå¯ä»¥æ²¡æœ‰ï¼‰**ã€**æ˜¯å¦ä½¿ç”¨åŠ é€Ÿ**ï¼Œ**å¦‚æœç”¨çš„è¯ï¼Œç”¨å“ªå‡ ä¸ª**ã€**æ¨¡å‹è®­ç»ƒå‡ è½®**
        
        ä»¥åŠæ¯è½®epochä»¥åï¼Œæ˜¯å¦ä½¿ç”¨callbacksï¼ˆä¿å­˜æ¨¡å‹å’Œlearning rateï¼‰
        
        <aside>
        ğŸ’¡
        
        callbackséå¸¸é‡è¦
        
        </aside>
        
        è¿˜æœ‰å°±æ˜¯è®¾ç½®loggerçš„ä¸¤ä¸ªç»†ææœ«èŠ‚çš„ä¸œè¥¿ï¼š`logger._log_graph`å’Œ`logger.__default_hp_metrics` 
        
    - å†™å¥½trainerä»¥åï¼Œå°±éœ€è¦ç”¨fitå‡½æ•°å°†trainå’Œvalid dataloaderä¼ å…¥è¿›å»äº†
    - æ¨¡å‹è®­ç»ƒå®Œæˆä»¥åï¼Œè°ƒç”¨Lightning Moduleçš„`load_from_checkpoint(trainer.checkpoint_callback.best_model_path)`
        
        å‡½æ•°ï¼Œè°ƒç”¨è¿™ä¸ªæ¨¡å‹ï¼Œå¾—åˆ°æœ€å¥½çš„æ¨¡å‹